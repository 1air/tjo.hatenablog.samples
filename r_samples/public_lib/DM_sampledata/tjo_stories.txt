「データサイエンティストというかデータ分析職に就くためのスキル要件」という話題が某所であったんですが、僕にとって馴染みのあるTokyoR界隈で実際に企業のデータ分析職で活躍している人たちのスキルを眺めてみるに、

みどりぼん程度の統計学の知識
はじパタ程度の機械学習の知識
RかPythonでコードが組める
SQLが書ける

というのが全員の最大公約数＝下限ラインかなぁと。そんなわけで、ちょろっと色々与太話を書いてみます。なお僕の周りの半径5mに限った真実かもしれませんので、皆さん自身がどこかのデータサイエンティスト（）募集に応募して蹴られたとしても何の保証もいたしかねますので悪しからず。


統計学の知識は「みどりぼん以上」


みんな大好きみどりぼん（緑本）。初歩の初歩の線形モデルから、一般化線形モデル、混合効果モデル、そして階層ベイズとMCMCによるパラメータ推定、さらには空間統計モデルという、統計モデリング周り一式をこれ一冊でRコードと共に学べる名著*1です。これ1冊しっかり手を動かして勉強すれば、確実に統計モデリングの最低レベルをマスターできると思います。


実際のビジネスシーンでのアドホック分析はビジネスサイドの人たちの天然なリクエストによって想像以上の難題に発展するケースが多く、意外と階層ベイズをやる羽目になることも多いのです*2。ということでみどりぼん以上の知識とスキルを持っていることが統計モデリングにおける最低限の要件ですよね、ということで。


機械学習の知識は「はじパタ以上」


これまたみんな大好きはじパタ。本当に基礎からベイズ則・識別関数・kNN・パーセプトロン＆NN・SVM・部分空間法・Ward法・K-means・混合正規分布(EM)・アンサンブル学習とランダムフォレストさらには正則化、という感じで教師ありorなし学習の諸手法がバランスよく紹介されていて、その上Rによる出力例もついていて非常に分かりやすいです。


この本の良いところは最低限知っておくべきアルゴリズムの詳細がつかめる程度にはきちんと数式も載っていて、機械学習の鬼門でもあるチューニング周りについて必要な知識も手に入るところかと。実際問題として、ビジネスの現場で機械学習を実装するというとフルスクラッチで書くにせよ、ライブラリ・パッケージを使うにせよ、チューニングが最重要になることが多いので*3その辺の知識を身に付けていることは極めて大事です。


これらのことを鑑みるに、はじパタ以上の知識とスキルを持っていることが機械学習における最低限の要件と考えても良いのではないかと思います。


コーディングスキルは「プロダクトレベルでなくても良いがせめて前処理コードは自分で書けるぐらい」

ぶっちゃけ僕は目下のところ全くシステム業務やってないので偉そうなことはこれっぽっちも言えないんですが*4、いかな他にエンジニアがいようともデータの前処理はどうしても自分でやらなきゃいけません。大抵のデータはアプリサーバのアクションログみたいな形で蓄積されているので、これを片っ端から機械学習や統計モデリングに使えるようなデータテーブルの形に直していくようなバッチ処理は必須と言って良いでしょう。


使う言語は何でも良いと思いますが、普及度とパッケージの充実度と実装のしやすさとロジスティクスの統一と最先端の機械学習手法への対応ぶりを鑑みるに、Pythonが第一選択かなと。その次がR。ただしバッチ処理を書くのには全くお薦めしません。後は王道のJavaを皮切りに、アプリ向けのRuby、そしてC++, C#などなどってところでしょうか。別にGoでもHaskellでも良いとは思いますが。。。


いずれにせよ、本職のエンジニア仕事も請け負わない限りはプロダクトインできるほどの完璧なコーディング力は必ずしも要らないとは思うものの、せめて自分や同僚向けのインターナルな用途のバッチを書ける程度のコーディングスキルは必要ですね。


SQLはそもそも書けないとその時点で詰むかも

この地上にDBを持たないデータ分析部門は存在しないと思ってほぼ間違いないので、DBを触れることはデータ分析のプロたり得るための最低条件と言っても良いと思います。というか、DBを触れなければ自分でデータを持ってくることはできないし、HDFSや（例えば）AWS Redshiftみたいなのを使わないと回らないような超大規模データが相手だと前処理もできないということにもなり、マジで詰みますので。


で、DBを触るということは当然クエリが叩けなければならないわけで、クエリを叩くにはSQLの知識が要りますよねということで、SQLが書けるというのがここでの最低限のスキルと言って良いでしょう。もちろんPostgreSQL, MySQL, Hive, 某O, 某Iとか方言はありますが、どうかなー。。。SQL99以上のものが書ければ大丈夫かなと。


ちなみにDBを自分で立てられないor立てるのが面倒という人は、MS AccessでSQL99みたいなクエリを書いて勉強するという方法もあるので参考までに～。


これらは「最低限」のスキル要件



冒頭にも書いたように、あくまでも「これだけ出来ればデータ分析職としてスタートラインに立てますよね」という最低限のラインを意識してますので、基本的にはこれだけでは実務で舞い込んでくるビジネスデータ分析課題をさばくのはつらいと思います。どちらかというと、これらの最低限のスキル要件を土台として、さらに勉強してより高度なスキルを身に付けていけることが重要なんじゃないかなぁという気がしています。


例えばですが、アルゴリズム実装系なら今大流行のDeep Learning (Deep Belief Net -> ConvNet)とか、アドホック分析系ならMC(MC)サンプラーを駆使したベイジアンモデリングとか、効果測定とかであれば傾向スコアなどに基づく因果推論とか、もちろんトピックモデル以下果てしなく裾野の広がる自然言語処理とか、学術的にもゴツくてビジネスの現場でも今後導入が見込まれる分野はたくさんあります。


ちなみにこの「最低限の要件」に対して「レベルが高過ぎる」「低過ぎる」の反応が拮抗していた感じだったので、カットオフラインとしては妥当なんじゃないでしょうか（笑）。


以上に挙げたのは、勉強し続けてそういう「さらなる高み」に登るための足掛かりとしての「最低限のスキル要件」なのだ、と見るべきだというのが僕個人の意見です。


このブログも炎上ラーニング（笑）を通じて、そういう勉強を僕が進めていくためのものなんですよー。ということでお粗末様でした。。。


追記1

上のピラミッドの中身とか順位とかに色々突っ込まれてますが、昔の記事の使い回し＆ぶっちゃけ適当なので。。。とは言えノンパラベイズ（トピックモデル）やフルベイズ系が閾値より下とも思えないので、閾値の上か下かだけをご覧いただき、「順番」は気にしないでいただきたく、ということで。ってかどの分野が上か下かとか言い出すと宗教戦争になる悪寒。。。


あーでも、みどりぼんをmustにすると左側のピラミッドはてっぺんが閾値になりかねないような。。。（汗）


追記2




先週のブクマ数首位でした（白目


追記3
データアナリストに多分必要な物。

データ分析業界のノルウェーの鮭の生態学な優秀な友人がナイスな反応記事を書いてくれました。当たり前過ぎて僕が忘れかけていた大事なことをうまくまとめてくれているので、よろしければ是非お読みください。

*1:現在の版ではWinBUGSのみ対応でStanに対応していないのが珠に瑕ですが。。。

*2:トレンドと季節調整と干渉変数とレジーム転換が同時に入るとかザラ

*3:ありがちなのがSVMのcostとgammaをチューニングしろとか、ランダムフォレストのmtryをいじれとか、後は何と言ってもNN or DNN or CNNのレイヤーパラメータをいじれとか

*4:かつては僕もHiveだらけのバッチとcrontab書いてましたよ、一応当時はエンジニア扱いされてましたから。。。

データサイエンティストブームが去りつつある一方で、データ分析ブームそのものはじわじわと広がり続けている感じのする昨今ですが。最近また、色々なところで「本当にビジネスやるのに統計学って必要なの？」みたいな話題を聞くことが増えてきたので、何となくざっくりまとめて書いてみました。


ちなみに今回の話題の参考図書を挙げようと思ったら、この辺ですかね。



本当は赤本を読んで欲しいんですが、赤本の6ページ目*1ぐらいから体が拒否反応起こす人も出てくるおそれを感じるので、とりあえず弁当屋本*2から入った方が無難かもですね。。。


統計学的検定は「分からないなら使わない方が無難かも」

あまり統計学に明るくない人ほど、「統計学＝仮説検定」というイメージを抱いていることが多い印象があり、よりによってそういう人ほど仮説検定の考え方が理解できなくて戸惑ってしまうのがもはや鉄板パターンではないかとすら思われます。


赤本には「仮説検定も人間の論証感覚を定式化したもので、ごく自然で理解しやすい」(p.233)と書いてありますが、僕には到底そうは思われないんですよね。これまで色々な人に統計学について教えたりアドバイスしてきた個人的な経験から言うと、むしろ仮説検定のところで理解がこんがらがって、統計学そのものの習得につまずく人の方が多い気がします。


では仮説検定とはそもそも何なんでしょうか？　細かいことはこの記事の最後にまとめてありますが、それはそれとして同じp.233の章頭には、こう書いてあります。

「仮説検定」は、統計的仮説の「有意性」の検定である。仮説の下でわれわれが期待するものと、観測した結果との違いを、これらの差が単に「偶然」によって起ったものか否かという見地から、確率の基準で評価する。

これを読んで「そうだよね」と言える人は大丈夫です。でも、これを読んでもポカーンとしてしまう人にはもう既に仮説検定は無理ゲーではないかと。


そういう無理ゲーな人が頑張ってExcelのttest関数やRを使って統計学的検定をやってみたとしても、その結果をどう解釈すべきか？というところでつまずいてしまうのではないでしょうか。p < 0.05だから良いのか？とか、p < 0.0000001だからもっと良いんだっけ？みたいな、そういう話になってしまうような。


しかもそこで追い打ちをかけるのが、例えば有意ではないという結果になった時にそれが「実際に帰無仮説が真」なのか「単にサンプルサイズが小さくて検出力が足りないだけ」なのか判断せよという問題。前者なら果てしなくサンプルサイズを大きくしても有意にはならないし、後者なら今度は効果量(effect size)*3のことを考えなければいけません*4。


極端な話、「クリエイティブAの方がBより統計的に有意に効果があるけどCTRの増分が0.1%しかない」だったら、はたしてAにかけるコストに見合うかどうか考えざるをえないですよね？


そういったことをある程度理解した上で統計学的検定を使えるならまだしも、p値の高い低いぐらいしか分からないような人が無理に使うのはかえって混乱のもとになるだけかもしれません。ならまぁ、そういう人はやらなくてもいいんじゃないかなぁと。代替手段としてこんな提案を前にもしたことがありますが。




タイトルにもあるように全然厳密ではないんですが、それでもExcelでも簡単にできるし、やろうと思えばその辺のBIツールのカスタマイズでやれなくもないし、良い話だとは思うんですよね。ひとつのソリューションということで。


ぶっちゃけて言えば、統計学的検定についてよく分からない人は「平均値」と「標準偏差（or分散）」が分かれば十分だ*5、とも言えます。それらに拠るだけでも、それなりの結果は得られますので。


でも可能な限り統計学的検定は使うべき

とは言え、統計学的検定の考え方をある程度理解できる人であれば*6、やはり検定は使った方が良いです。それはいつも勉強材料にさせていただいているこちらのブログでも指摘されている通りかと。

統計学的検定に対するある拒絶反応: ニュースの社会科学的な裏側

もちろん、統計学的検定はそれ自体が一種の科学方法論みたいなもので取り扱いが難しいんですが*7、一方で正しく使えば「偶然ではない本当の必然」にたどり着くための良い助けになります。


そしてそれ以上に大きなメリットとして、「毎回データを自分の目で見なくても済む」ということがあります。上に挙げたような「プロットにして目で見て判断」だと全てのデータを見て行かなければいけないわけですが、もし監視するデータが例えば100セットとかあったら大変ですよね（笑）。できればどれもt検定やらカイ二乗検定やら順位和検定やらである程度自動的かつ機械的に判定して「有意に動いたor動かない」だけをインジケータにして見られるようにしておけば、いちいち100個全てのプロットに目を通さなくても良いわけです*8。


多変量解析までは出来ると嬉しいことが多いかも

これまた上記引用ブログ記事でもコメントされていますが、一般にクリエイティブ改善の現場では純然たる二者比較であるA/Bテスト単体よりも、多要素での多測定値間での比較をするケースが結構多いのではないかと思います*9。


そういう場合は、普通に多変量解析（重回帰分析＝正規線形モデルもしくは一般化線形モデル）を用いる方が手っ取り早く、なおかつ確実に統計学的にもっともらしい結果を得やすいことでしょう。特に、多少あいまいでも要素間（クリエイティブ要素の色・文字サイズ・フォントなどなど）でそれなりに「独立」とみなせる状況であれば*10、多変量解析にかけてしまった方が一括で「どのクリエイティブ要素が最も効果的か」を出すことができるので、効率的だともいえるでしょう。


そして、大半の多変量解析は「推定」を行うことができます。どちらかというと「AとBとでどちらが強いか」という二択ではなく、「A・B・C・Dの強さ*11はそれぞれこれくらいある*12」というように量的（定量的）な結果として得られるので、例えば大小関係から優先順位をつけるみたいなこともできます。「推定」の話は例えばこの辺に。



こんな感じで、要素間の大小＋その信頼性がいっぺんに手に入るので、ややこしい複数要素間での比較もスッキリと片付けることができます。もちろんそんなに単純にいかないケースも多いのですが、これが出来ることでだいぶこの手の検証作業は効率的になるはずです。


ただし多変量解析から先は専門家に投げた方が無難

とは言え、多変量解析から先の領域は結構複雑です。単なる重回帰分析＝正規線形モデルであればまぁ難しくない方だとは思いますが、これが一般化線形モデルの領域に入ってくると色々ややこしくなります。ややこしくなって僕が勉強し直す羽目になった例がこちら。

「使い分け」ではなく「妥当かどうか」が大事：重回帰分析＆一般化線形モデル選択まわりの再まとめ - 銀座で働くデータサイエンティストのブログ

調べていくうちに、定番の教科書の不備な点まで見つけてしまうという面倒くささ。なので、一般化線形モデルでも割と多用されるロジスティック回帰にしても例えばExcelではもはや普通にはできないし*13、かといってRでコマンドだけ覚えたからといってその結果の解釈も難しくなるわけで*14。ましてや、一般化線形モデルでも追い付かず混合効果モデルとか階層ベイズモデルとか、それこそMCMCサンプラーとか使うようになったらそんなレベルでは済まないという。


さらに、単なる測定データというだけならともかく、マーケティングデータだと社会科学系・ファイナンス系データと同じように、同時性・系列（自己）相関・不均一分散といったデータ系列自体のややこしい特性とも向き合わなければなりません。


そういう領域まで来たら、もう詳しい人に投げてしまった方が無難だと思います。。。もちろん詳しい人が手近にいることが前提ですが（笑）。最近はスポットで相談に乗ってくれるデータ分析コンサルも結構あるので、経費さえ問題でなければ迷わずそういうところに聞いた方が早いかも。


統計学を用いることは、究極的には属人的スキル依存からの脱却につながる

では、改めて「何故ビジネスの現場で統計学を使うのか？」という点について。これは昨年8月の講演会でお話した内容をちょっと改変して書いてみると*15、

統計学はものすごいことを発見すると思われがちだけどそんなことはない。

データさえあれば、誰でも（分析結果を）再現できるようにするのが統計学を用いて分析することのメリットであり、長年のベテランとの知見と照らし合わせて答え合わせができる。

あるいは、まったくの新しいサービスや商品で、今までベテランの暗黙知がない場合には統計学が役立つ。

ということで、ベテランの暗黙知のような属人的スキルに頼らずともデータ分析の結果を再現することができるというのが、統計学を用いるメリットだと僕は思うのです。データが多くてとても人手をかけて全部目視するわけにはいかない状況であったり、長年のベテランが退職してしまって代わりがいないという状況であっても、ベテランの暗黙知と同じ結果を返すような統計学的データ分析が確立していれば、それなりにうまくいくはずなのです。


その統計学の利点がメンバー全員の間で共有されている現場であれば、統計学を用いることには大きなメリットがあるのではないでしょうか。

今日何気なく呟いたツイートが、見ていたら結構RT&favされていた模様で。




この後も色々補足で呟いたんですが、せっかくなので簡単にまとめたものを書いてみました。これから社会人で統計学や機械学習を学ぼうと考えている人の参考になれば嬉しいです。


あ、これはベタな言い方をすれば「データサイエンティスト（死語）になるにはどうしたら良いか」にもつながる話なんですが、ここではもっと広く「統計学や機械学習を使う仕事をしたいと思ったらどう独習するべきか」という話にしておこうと思います。


「落下傘方式」＝必要になった時に必要な項目だけ学んで実践する

これは僕のオリジナルのアイデアではなく、まだ大学受験生ぐらいの頃に読んだ野口悠紀雄氏の著書か何かに出ていた喩えです*1。すなわち、とにかく「必要になった時に必要な項目だけ学び、覚えたらとにかく実践してみる」というやり方です。





例えば「今目の前に顧客の行動データがあってどのアクションをしてもらうと購入金額が増えるかを知りたいんだけど。。。これは重回帰分析って方法で簡単に求められるのかな」みたいな。そこでまず重回帰分析（正規線形モデル）を統計学の本を開いて学び、RやPythonでパッケージなりライブラリを用いてサクッと計算してみてその出力や癖を覚え、実践してみる。そしてまた何か疑問点があったら本を読むところに戻って調べ、また試す。これを繰り返すというわけです。


この繰り返しで、広大な統計学と機械学習のマス目を少しずつ埋めていけば、歩みは遅くても着実に理解が深まっていくと思います。これを、普通に大学院で専門的かつ網羅的に勉強するのと同じようにやろうとすると、こんな感じに見えるかもしれません。



高くそびえる、どこまで登ったら頂上にたどり着くか分からないような険しい山が2つ屹立しているように見えるかもですね。これだとあまりにも「いくらやっても果てしない感」が強過ぎて、しんどいと思うんですよ*2。落下傘方式の方が、多少いい加減でもじわじわと統計学と機械学習が身に付くはずです。


はじめに網羅的なテキストを流し読みで良いので通読する

ただ、落下傘方式一辺倒だとやっぱり全体の見取り図が分からなくて困ったり理解が曲がった方向に向かっていってしまったり、それこそ落下傘で降りたところの浅い理解のままで押し通してしまうことも多いと思うので、はじめに見取り図というか大きな地図がつかめるような本を流し読みで良いので通読すると良いでしょう。テキストはいくらでも良いものが挙げられますが、個人的には以下の2点ですかねー。




もちろん、腕に覚えのある人は黄色い本*3とかでも良いと思います（笑）。大事なことは、細部は初めのうちは分からなくて覚えられなくても良いので、全体の流れというかイメージだけは頭に入れておくこと。そうすることで「引き出し」が身に付くし、何か知りたいことがあった時にどこから手を付ければ良いかが分かるようになるはずです。


例えば「二値データの回帰」となった時に、きちんとすぐ正規線形モデルではなく「ロジスティック回帰（ロジットorプロビットモデル）」という選択肢が思い付くかどうかというのは、結構大事だと思うのです。


ある程度知識が増えたら、それらの知識の関連性も学ぶ

「落下傘方式」がいかに強力だとはいえ、勉強が進めばそれなりに覚えたことも多くなってきて次第にこんがらがってくると思います。そういう時に、自分の「理解」の「体系化」を、実際の学術体系と照らし合わせながら進めていくことも大切です。つまり、「覚えた知識同士の関連性を学ぶ」ということですね。


例えば、ロジスティック回帰は統計学においては一般化線形モデルの中で最尤法とともに学ぶものですが、機械学習においては出力を確率値として与えられる識別モデルの一環として学ぶもの。PRMLなんかを見れば分かりますが、ベイズの枠組みの中で理解することも可能です。このように、同じひとつの事由であっても、多くの切り口から理解することができます。そこで「結局これって尤度計算なんじゃん→MCMCでいけるよね」みたいな流れにまで行けば*4、なかなか良い発展なんじゃないかと。


研究の道に進みたければ、一度戻ってゼロから体系的に学ぶべし

とは言え、これはあくまでも統計学や機械学習の「ユーザー」という立場における勉強法。もし、例えば社会人博士などで改めて数理統計学や機械学習アルゴリズム研究の道に進みたいと思ったら、こんな程度で満足していてはいけません。何故なら研究というのは「既にあるものを知る」ことではなく、「まだそこにないものを見出すor創る」ことだからです*5。そのためには、基礎となる学識体系を自分の中に築き上げる必要があります。


なので、そういう人にはまだるっこしいと思っても一度ゼロから徹底的に体系全体を学び直すことをお薦めします。できれば、隣接する別分野の勉強もすると良いでしょう。それこそWAICの例のように*6、例えば代数幾何学みたいなところも勉強した方が後々役に立つかもしれませんし。そうすることで、高い山に登るための基礎体力がつくはずです。それが備われば、もっと高い山＝研究目標を乗り越えることもできるようになるでしょう。


最後に

そんなわけで、僕も細々と独習を続けています。。。今は専らベイジアンばっかりですが（笑）。そのうち本格的に計量経済学とかも勉強しようかなぁ。

何かこんなメディア記事が出ていたようです。





これを読んで色々な人がツッコミを入れまくっている模様ですが、この記事の不思議なところは「完全に間違った説明というわけでもないのに何故か（両分野に詳しい）誰が読んでも猛烈な違和感を覚える」ところなんじゃないかなぁと。


正直、これはライター・インタビュアー・コメンテーター・編集者の誰のせいなのかは全く分からないんですが、ツッコミ入れられまくっている内容について色々あげつらってもあまり建設的でないので、ここでは記事中で本題として取り上げられている「統計学と機械学習の違い」についてちょっとコメントしてみようと思います。


あ、もちろん僕がこれから書くコメントも別に正しいとは全く限らないので、おかしいところや間違ってるところがあったらバンバン突っ込んでいただければ幸いです*1。そしてガチ勢向けのコメントでもないので何卒悪しからず。


統計学はデータを「説明」することにより重きを置く

昨年出版した拙著でも統計学的な分析の代表例として「検定」*2「回帰によるパラメータ推定」*3を取り上げたわけですが、一般に統計学はそのデータがどういうものであるかを「説明」することに重きを置くものだと僕は理解しています。


即ち「検定」であれば、例えば平均値の差に関するt検定であればA群*4とB群*5との差があるかどうかを「説明」することになるし、ANOVAであれば群をまたいで実験条件間*6での差があるかどうかを「説明」することになります。他の例えばカイ二乗検定にせよ、順位和検定にせよ、やはり何かしらを「説明」する手法です。


そして重回帰分析（正規線形モデル）を含む線形モデル族ではモデル推定の結果として偏回帰係数（パラメータ）が求まるわけですが、それらのパラメータは個々の説明変数*7にどれくらいの目的変数*8への影響度があるか*9を「説明」するものでもあります。



なればこそ、例えば線形モデル族では偏回帰係数が有意に0から離れた正負いずれかの値を取るか否かがよく論じられるわけです。要は上の図に出てくる個々のβの大小が重要ということで。


一方、線形モデル族では「目的変数＋説明変数」の組み合わせでデータを扱うことから、特に説明変数が外生変数*10であれば純粋な「予測」も可能なわけですが、意外にも統計学の教科書はあまり予測について取り上げていないことが珍しくなく、例えば東大出版会の定番青本（自然科学の統計学 (基礎統計学)）では一言も予測については触れておらず、緑本（人文・社会科学の統計学 (基礎統計学)）でもほんの数ページほど回帰分析における予測について述べている箇所があるだけです*11。RやらPythonやらの統計分析系パッケージでは当然のように予測はできるんですけどね。。。


むしろ東大出版会青本・緑本とも偏回帰係数の検定などなどにページが割かれており、また「当てはめ」の良し悪しということについての記述も多く、そういうところから見てもやはり統計学は「説明」により重きを置く体系なのだなという印象を持っています。


実際、モデル評価にはどちらかというと学習データのみからでも計算可能な*12AICなどの指標などが多いということもあり、未知データへの適用よりも学習データに対する「説明」が重視されていると見て良さそうです。


機械学習はデータから「予測」することにより重きを置く

巷でよく言われるのが、機械学習と言えば「Yes/Noの二値や1, 2, 3...といったカテゴリに分類する」ためのモデルを作るもの、という理解かなと。かつてならスパム分類、今ならヒト・イヌ・ネコといったカテゴリへの画像分類といった事例が人口に膾炙しているせいもあり、多くの人々がそのように理解していても不思議ではないですね。ただ、それはあまり正確な理解ではない気がします。


教師あり学習

例えば多くの教師あり学習分類器は、目的変数をカテゴリ型から連続値を持つ数値型に替え、さらに損失（誤差）関数などを変更することで「回帰器」としても学習させることができます。SVMもランダムフォレストも一般には二値分類器としてばかり捉えられがちですが、どちらも回帰器として学習させることが可能です。もちろん今流行のDeep Learning (DNN)も、Xgboostも、回帰器を構築できます。なので、機械学習は常にYes/Noの二値のようなカテゴリ分類しか学習しない、回帰はやらない、というのは正しくないわけです。


そして重回帰分析とロジスティック回帰が多くの機械学習の教科書でも取り上げられているように、線形モデル族も基本的には機械学習の一分野としてカウントされ得ます。もちろん、その中では普通に偏回帰係数のようなパラメータの推定も行われます。そういう意味においては、機械学習でも手法によっては「説明」の要素を持っているとも言えます。それこそランダムフォレストやXgboostも変数重要度の計算をやれるわけで。


しかしながら、機械学習の最大の特徴はやはり「未来値の予測」にあると言って良いでしょう。




上の図で言えば、赤い点*13と青い点*14から成るデータに基づいて紫の線で描かれるような分類モデルを学習する、というのが機械学習の本義なわけです。この紫の線がうまく描ければ、新たなデータとして別の点*15が追加された時に、その点が赤or青のどちらか*16を正しく分類できるはず！というのが機械学習における「予測」の思想です*17。


ただ、線形モデル族のようにヒトが直感的に理解できる形でモデルが出来上がるとは限らないのが機械学習の特徴で、そこが統計学との大きな違いかもしれません。


例えばSVMであれば分離超平面はサポートベクターによって与えられ、そこには偏回帰係数のような綺麗に定式化されたモデルはありません。ランダムフォレストなら山のような決定木からなる「森」ですし、Deep Learningに至ってはレイヤーごとに表現が圧縮されてしまうので線形モデルのような定式化もへったくれもありません*18。


このように「説明」を多少犠牲にしてでも「予測」の精度を上げたい、というのがおそらく機械学習の思想なのだと僕は理解しています。


その究極形がいわゆる「オンライン学習（ストリーミング学習）」かな、と。学習データをまとめて読み込んでモデルを作る＆新たにデータが加わった場合はまた一から読み込み直す「バッチ学習」とは異なり、「オンライン学習」では過去に計算したモデルを（例えば線形モデル族などの）パラメータのみで表現可能なようにしておき新たにデータが加わった場合はそのデータだけを読み込んでパラメータのみを更新する、というやり方をするわけです。イメージとしては、よく引き合いに出されるメールの「重要」ラベル分類かと*19。このやり方だと過去のデータは全部捨ててしまうこともできるため＆現在のパラメータが最近のデータの影響を受けやすいため「説明」には必ずしも適さない一方で、「予測」を計算負荷を抑えた上でうまくできるということになります。


なお機械学習でのモデル評価というと、cross validationやhold-outのようにそもそも学習データとは別に（何かしらの方法で）テストデータを作ってその予測精度を見るというものが多く、この点でも機械学習は「予測」により重きを置く*20体系なのだなという印象があります。


余談ですが、同じような評価は教師あり「回帰」器でもできます。ただ、そもそも機械学習系の回帰器は統計モデリングによる回帰モデルに比べると様々なばらつき要因に対して弱いかなぁという印象も。。。*21


教師なし学習

これまで散々教師あり学習の話をしてきましたが、教師なし学習（クラスタリングや広義に見て因子分解系など）も実務上は最終的に「何かしらのクラスタに新たにやってきたデータを振り分ける」のが目的になりがちなので、やはり「説明」よりは「予測」に重きを置くという印象を持っています。


例えばK-meansのような手法で学習データ*22をk個のクラスタに割り振ってから、それぞれの特徴量*23に添えて改めて1,...,kなるカテゴリを学習データとして与え、これを教師あり学習の多クラス分類器にかけて分類モデルを作り、未知データ*24をk個あるクラスタのいずれかに割り振るという発想は、割と巷ではよく使われている印象です*25。


というわけで、教師なし学習であってもやはり「予測」に重きを置くことになるケースが多いというのが僕の理解です。ちなみに教師あり学習と組み合わせない場合でも、そもそも例えばクラスタリングであれば出たとこ勝負で目の前のデータをk個にまとめることになるし、レコメンドであれば新たにやってきたユーザーに対して何かしら好んでくれそうなアイテムを推薦することになるので、どのみち「予測」をやることになる気もしますが。


とは言え、統計学と機械学習の違いは基本的にはそれほど大きくないし互いに重なる部分だらけ

色々統計学と機械学習との違いをあげつらってきましたが、ぶっちゃけこの両者は「違う」というより「重なる」部分の方が多いというのが個人的印象です。


まず、統計学であろうと機械学習であろうと、モデルを構築するためには学習データが必要です。特に統計学の視点から線形モデル族を構築する場合と、機械学習の視点から学習モデルを構築する場合とでは、どちらであってもそれ相応に十分な量のデータを貯めてモデル推定に回さなければなりません。つまり、統計分析でも機械学習でもモデルを作りたければ学習データを大量に貯めなければいけないことに変わりはないわけです*26。


なので、統計分析と機械学習とでデータの扱いが異なるということはありません。ぶっちゃけ、システム基盤を組んでの実務の現場では、アドホック分析として統計分析を行う場合でも、システム実装として機械学習を実践する場合でも、いずれにせよ膨大なトランザクション*27から必要なデータを抽出してきてDBに貯め、これを分析のために適宜抽出して分析基盤に回すための仕組みが必ず求められます。そういうわけで、ITシステム基盤として見れば、どちらであっても必要なインフラにそれほど差はない気がしています。


次に、線形モデル族が統計学と機械学習の双方に重なっている点からも分かるように、数理的基礎という意味では両者はむしろ密接な関係にあると見るべきです。特に機械学習においては統計学に近い方法論に基づいてモデル推定などなどを行う手法は「統計的学習」と呼ばれ、そもそも『統計的学習の基礎』という機械学習の百科事典的大著までもがあるわけで。



統計学にせよ、機械学習にせよ、根底にあるのはひとまず「目の前にあるデータをモデリングすること」「そのために『ばらつき』をうまく扱うこと」の2点かな、と個人的には思っています。そのモデルを「説明」に使うのか、「予測」に使うのか、の思想が異なるのが統計学と機械学習の間の違いかなぁと。そこさえ分かっていれば概ねデータ分析の現場でもうまく使いこなせるようになるんじゃないでしょうか。


そう言えば僕は階層ベイズ以外ではあまりベイジアンは使わない（特に機械学習だと殆ど使ってない）のであまり知ったかぶりはできないんですが、ベイジアンは割と統計学と機械学習とで重なりが大きい領域かなぁと思ってます*28。ノンパラベイズの理論のところなんか読んでると尚更。


データ分析において大事なのは「統計学『も』機械学習『も』理解する」こと、ということで推薦図書など

ということで、統計学「も」機械学習「も」理解するように努めましょう！というのがこの記事で言いたかったこと。そこで推薦図書を紹介したいのですが、まず統計学をコードを書いて計算結果を見ながら学ぶにはこちら。Rで一通り試してみながら、一般的な統計学のイロハをやはり過不足なく学ぶことができるはずです。



一方、統計学はある程度勉強したという人が機械学習についても理解したいというのであれば、最低でもこちらの本を一通り読んでおくべきでしょう。先日このブログでも大絶賛したばかりですが、この本には初心者が機械学習を学ぶ上で必要な全てが過不足なく詰まっています。RでもPythonでも*29実践できるので、どの開発環境の方にとっても有益な本だと思います。というか、機械学習について知ったかぶりがしたいなら最低でもこの養成読本・機械学習入門編の特集1ぐらい読め！ってところですかね（笑）。



そう言えばバランス良く統計分析と機械学習の両方を同時に学べる本ってほとんどない気がするんですが、強いて言うならRで統計学＋機械学習という一連の流れを学ぶにはこちらを。



はい、例の昨年出版した拙著ということで手前味噌でごめんなさい（笑）。基本的に説明が平易*30なので、まず拙著を通読＆Rコード写経していただくというのが良いかなと。ある程度統計学・機械学習それぞれのコンセプトについても簡潔に説明してあるので、あくまでも導入としてお読みいただければ幸いです。

*1:いつも通りの炎上ラーニング

*2:3章

*3:4章

*4:例えばDB基盤Aの何かしらのレイテンシ

*5:DB基盤Bのレイテンシ

*6:例えばユーザーグループA, B, C, D, Eそれぞれにスマホ広告1, 2, 3, 4, 5を表示し分けるとか

*7:例えば広告企画a, b, c, d...

*8:CV数or売上高など

*9:「弾性率」みたいな捉え方をされることもありますね

*10:広告出稿数、広告予算など当事者の側が操作可能な変数

*11:その代わり計量時系列分析の項では予測に関してスペースを大きく割いているが、そもそも線形モデルではない

*12:尤度を用いて解析的に、という意味で

*13:例えばCVしたECサイトユーザー

*14:CVしなかったユーザー

*15:つまり説明変数は分かっているがまだCVするかどうか分かっていないユーザー

*16:CVするorしない

*17:もっともこれの元ネタはXORデータセットの単純3層NNによる分類で、しかもご覧の通り精度はかなり悪そう（笑）

*18:もっともCNNだとレイヤーごとに解像度ごとの表現が取れたりするので別の意味で綺麗という

*19:メールが新たに来るたびにモデルパラメータを更新し、それ以降過去のメールは参照しない

*20:CVが用いられることが多いという意味ではAICで評価するよりもさらに実際的に汎化性能の良し悪しをチェックしているとも言えます

*21:特にSVMだろうとランダムフォレストだろうと非線形トレンドのあるデータには機械学習回帰器はめっぽう弱い

*22:例えば広告面

*23:テキストから生成した素性

*24:新たに獲得した別の広告面

*25:オンライン中華レストラン過程みたいに未知データを次々と既存or新規クラスタに割り振っていく手法もあるにはありますが

*26:オンライン学習でも実務上は最初のステップではある程度学習データを貯めてモデル推定を行い、それなりに最初から予測精度の高いパラメータにセットしておく必要がある

*27:例えば会員制サイトの膨大な取引データや会員のアクションログなど

*28:Gibbs Samplerは統計モデリングでパラメータの事後分布を求めるためのMCMCでも、ノンパラベイズ系クラスタリング手法の逐次サンプリング更新でも使われたりする

*29:果てはJuliaやSpark MLlibでも

*30:単に数式を端折ったという意味で